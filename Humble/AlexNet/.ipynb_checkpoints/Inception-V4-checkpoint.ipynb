{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breeding-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Input, concatenate, Dropout, Dense, Flatten\n",
    "from keras.layers import MaxPooling2D, Conv2D, AveragePooling2D, BatchNormalization\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "def block_stem(inputs):\n",
    "    net = conv2d(inputs, 32, (3, 3), strides=(2, 2), padding='valid')\n",
    "    net = conv2d(net, 32, (3, 3), padding='valid')\n",
    "    net = conv2d(net, 64, (3, 3))\n",
    "\n",
    "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\n",
    "    branch_2 = conv2d(net, 96, (3, 3), strides=(2, 2), padding='valid')\n",
    "\n",
    "    net = concatenate([branch_1, branch_2])\n",
    "\n",
    "    branch_1 = conv2d(net, 64, (1, 1))\n",
    "    branch_1 = conv2d(branch_1, 96, (3, 3), padding='valid')\n",
    "\n",
    "    branch_2 = conv2d(net, 64, (1, 1))\n",
    "    branch_2 = conv2d(branch_2, 64, (7, 1))\n",
    "    branch_2 = conv2d(branch_2, 64, (1, 7))\n",
    "    branch_2 = conv2d(branch_2, 96, (3, 3), padding='valid')\n",
    "\n",
    "    net = concatenate([branch_1, branch_2])\n",
    "\n",
    "    branch_1 = conv2d(net, 192, (3, 3), strides=(2, 2), padding='valid')  # different from the paper\n",
    "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\n",
    "\n",
    "    net = concatenate([branch_1, branch_2])\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def block_inception_a(inputs):\n",
    "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_1 = conv2d(branch_1, 96, (1, 1))\n",
    "\n",
    "    branch_2 = conv2d(inputs, 96, (1, 1))\n",
    "\n",
    "    branch_3 = conv2d(inputs, 64, (1, 1))\n",
    "    branch_3 = conv2d(branch_3, 96, (3, 3))\n",
    "\n",
    "    branch_4 = conv2d(inputs, 64, (1, 1))\n",
    "    branch_4 = conv2d(branch_4, 96, (3, 3))\n",
    "    branch_4 = conv2d(branch_4, 96, (3, 3))\n",
    "\n",
    "    return concatenate([branch_1, branch_2, branch_3, branch_4])\n",
    "\n",
    "\n",
    "def block_inception_b(inputs):\n",
    "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_1 = conv2d(branch_1, 128, (1, 1))\n",
    "\n",
    "    branch_2 = conv2d(inputs, 384, (1, 1))\n",
    "\n",
    "    branch_3 = conv2d(inputs, 192, (1, 1))\n",
    "    branch_3 = conv2d(branch_3, 224, (1, 7))\n",
    "    branch_3 = conv2d(branch_3, 256, (7, 1))  # different from the paper\n",
    "\n",
    "    branch_4 = conv2d(inputs, 192, (1, 1))\n",
    "    branch_4 = conv2d(branch_4, 192, (1, 7))\n",
    "    branch_4 = conv2d(branch_4, 224, (7, 1))\n",
    "    branch_4 = conv2d(branch_4, 224, (1, 7))\n",
    "    branch_4 = conv2d(branch_4, 256, (7, 1))\n",
    "\n",
    "    return concatenate([branch_1, branch_2, branch_3, branch_4])\n",
    "\n",
    "\n",
    "def block_inception_c(inputs):\n",
    "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_1 = conv2d(branch_1, 256, (1, 1))\n",
    "\n",
    "    branch_2 = conv2d(inputs, 256, (1, 1))\n",
    "\n",
    "    branch_3 = conv2d(inputs, 384, (1, 1))\n",
    "    branch_3_1 = conv2d(branch_3, 256, (1, 3))\n",
    "    branch_3_2 = conv2d(branch_3, 256, (3, 1))\n",
    "\n",
    "    branch_4 = conv2d(inputs, 384, (1, 1))\n",
    "    branch_4 = conv2d(branch_4, 448, (1, 3))\n",
    "    branch_4 = conv2d(branch_4, 512, (3, 1))\n",
    "    branch_4_1 = conv2d(branch_4, 256, (3, 1))\n",
    "    branch_4_2 = conv2d(branch_4, 256, (1, 3))\n",
    "\n",
    "    return concatenate([branch_1, branch_2, branch_3_1, branch_3_2, branch_4_1, branch_4_2])\n",
    "\n",
    "\n",
    "def block_reduction_a(inputs):\n",
    "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(inputs)\n",
    "\n",
    "    branch_2 = conv2d(inputs, 384, (3, 3), strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_3 = conv2d(inputs, 192, (1, 1))\n",
    "    branch_3 = conv2d(branch_3, 224, (3, 3))\n",
    "    branch_3 = conv2d(branch_3, 256, (3, 3), strides=(2, 2), padding='valid')\n",
    "\n",
    "    return concatenate([branch_1, branch_2, branch_3])\n",
    "\n",
    "\n",
    "def block_reduction_b(inputs):\n",
    "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(inputs)\n",
    "\n",
    "    branch_2 = conv2d(inputs, 192, (1, 1))\n",
    "    branch_2 = conv2d(branch_2, 192, (3, 3), strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_3 = conv2d(inputs, 256, (1, 1))\n",
    "    branch_3 = conv2d(branch_3, 256, (1, 7))\n",
    "    branch_3 = conv2d(branch_3, 320, (7, 1))\n",
    "    branch_3 = conv2d(branch_3, 320, (3, 3), strides=(2, 2), padding='valid')\n",
    "\n",
    "    return concatenate([branch_1, branch_2, branch_3])\n",
    "\n",
    "\n",
    "def conv2d(net, filters, kernel_size, strides=(1, 1), padding='same'):\n",
    "    net = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False)(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)\n",
    "    return net\n",
    "\n",
    "def create(classes_num=4):\n",
    "    inputs = Input((227, 227, 3))\n",
    "\n",
    "    # 227 x 227 x 3\n",
    "    net = block_stem(inputs)\n",
    "\n",
    "    # 4 x Inception-A ( Output: 35 x 35 x 384 )\n",
    "    for i in range(4):\n",
    "        net = block_inception_a(net)\n",
    "\n",
    "    # Reduction-A ( Output: 17 x 17 x 1024 )\n",
    "    net = block_reduction_a(net)\n",
    "\n",
    "    # 7 x Inception-B ( Output: 17 x 17 x 1024 )\n",
    "    for i in range(7):\n",
    "        net = block_inception_b(net)\n",
    "\n",
    "    # Reduction-B ( Output: 8 x 8 x 1536 )\n",
    "    net = block_reduction_b(net)\n",
    "\n",
    "    # 3 x Inception-C ( Output: 8 x 8 x 1536 )\n",
    "    for i in range(3):\n",
    "        net = block_inception_c(net)\n",
    "\n",
    "    # Average Pooling ( Output: 1536 )\n",
    "    net = AveragePooling2D((5, 5))(net)\n",
    "\n",
    "    # Dropout ( keep 0.8 )\n",
    "    net = Dropout(0.2)(net)\n",
    "    net = Flatten()(net)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(units=classes_num, activation='softmax')(net)\n",
    "\n",
    "    return Model(inputs, outputs, name='Inception-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "possible-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inceptionV4_model(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # validation split\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.1, random_state=10,\n",
    "                                                                    stratify=y_train)\n",
    "    print(len(X_train))\n",
    "\n",
    "\n",
    "    model = create()\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'],\n",
    "                  optimizer=keras.optimizers.SGD(learning_rate=0.01))\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_validation, y_validation))\n",
    "    \n",
    "    evaluation = []\n",
    "    evaluation.append(model.evaluate(X_train, y_train, batch_size=16))\n",
    "    evaluation.append(model.evaluate(X_test, y_test, batch_size=16))\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-turkey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 227, 3)\n",
      "4800\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 71s 208ms/step - loss: 1.0521 - accuracy: 0.5184 - val_loss: 19.3020 - val_accuracy: 0.2760\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.5166 - accuracy: 0.7893 - val_loss: 0.5850 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.3510 - accuracy: 0.8620 - val_loss: 0.3465 - val_accuracy: 0.8620\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.2258 - accuracy: 0.9193 - val_loss: 0.4367 - val_accuracy: 0.8385\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.2054 - accuracy: 0.9229 - val_loss: 0.1459 - val_accuracy: 0.9375\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.2042 - accuracy: 0.9261 - val_loss: 0.3947 - val_accuracy: 0.8802\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1582 - accuracy: 0.9436 - val_loss: 0.1571 - val_accuracy: 0.9427\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.1504 - accuracy: 0.9393 - val_loss: 0.1377 - val_accuracy: 0.9531\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.1144 - accuracy: 0.9570 - val_loss: 0.0916 - val_accuracy: 0.9688\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.1273 - accuracy: 0.9542 - val_loss: 0.2859 - val_accuracy: 0.9349\n",
      "216/216 [==============================] - 13s 59ms/step - loss: 0.3953 - accuracy: 0.9337\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.5569 - accuracy: 0.9167\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 52s 206ms/step - loss: 1.1308 - accuracy: 0.5259 - val_loss: 4.7652 - val_accuracy: 0.3542\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.6164 - accuracy: 0.7372 - val_loss: 2.5710 - val_accuracy: 0.7083\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.5366 - accuracy: 0.8048 - val_loss: 0.5212 - val_accuracy: 0.8307\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.3241 - accuracy: 0.8803 - val_loss: 0.5763 - val_accuracy: 0.7839\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.2996 - accuracy: 0.8871 - val_loss: 0.1752 - val_accuracy: 0.9401\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.2408 - accuracy: 0.9176 - val_loss: 0.2807 - val_accuracy: 0.9036\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.1705 - accuracy: 0.9423 - val_loss: 0.3326 - val_accuracy: 0.9010\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.1594 - accuracy: 0.9422 - val_loss: 0.3331 - val_accuracy: 0.8698\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.1266 - accuracy: 0.9524 - val_loss: 0.1628 - val_accuracy: 0.9349\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.1211 - accuracy: 0.9543 - val_loss: 0.0941 - val_accuracy: 0.9609\n",
      "216/216 [==============================] - 13s 58ms/step - loss: 0.0285 - accuracy: 0.9928\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0818 - accuracy: 0.9750\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 52s 206ms/step - loss: 1.0917 - accuracy: 0.5090 - val_loss: 18.4559 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.4683 - accuracy: 0.8076 - val_loss: 0.3032 - val_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.2828 - accuracy: 0.8883 - val_loss: 0.2871 - val_accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.2137 - accuracy: 0.9147 - val_loss: 0.2545 - val_accuracy: 0.9062\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.2229 - accuracy: 0.9174 - val_loss: 0.1142 - val_accuracy: 0.9557\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.1746 - accuracy: 0.9372 - val_loss: 1.7659 - val_accuracy: 0.8672\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1683 - accuracy: 0.9403 - val_loss: 0.2009 - val_accuracy: 0.9401\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 44s 204ms/step - loss: 0.1431 - accuracy: 0.9552 - val_loss: 0.0987 - val_accuracy: 0.9557\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 44s 204ms/step - loss: 0.1099 - accuracy: 0.9584 - val_loss: 0.1747 - val_accuracy: 0.9271\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.1079 - accuracy: 0.9632 - val_loss: 0.1213 - val_accuracy: 0.9531\n",
      "216/216 [==============================] - 13s 61ms/step - loss: 0.0476 - accuracy: 0.9829\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.1083 - accuracy: 0.9573\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 53s 209ms/step - loss: 0.9854 - accuracy: 0.5495 - val_loss: 2.2028 - val_accuracy: 0.5547\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 44s 204ms/step - loss: 0.5328 - accuracy: 0.7758 - val_loss: 0.4763 - val_accuracy: 0.7682\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 44s 204ms/step - loss: 0.3518 - accuracy: 0.8681 - val_loss: 0.1950 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2552 - accuracy: 0.9036 - val_loss: 0.1320 - val_accuracy: 0.9479\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2233 - accuracy: 0.9276 - val_loss: 0.1445 - val_accuracy: 0.9453\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1723 - accuracy: 0.9393 - val_loss: 0.1438 - val_accuracy: 0.9583\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1331 - accuracy: 0.9568 - val_loss: 0.1538 - val_accuracy: 0.9427\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1263 - accuracy: 0.9548 - val_loss: 0.1014 - val_accuracy: 0.9583\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1248 - accuracy: 0.9557 - val_loss: 0.0825 - val_accuracy: 0.9740\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1206 - accuracy: 0.9593 - val_loss: 0.2960 - val_accuracy: 0.8984\n",
      "216/216 [==============================] - 13s 61ms/step - loss: 0.1737 - accuracy: 0.9306\n",
      "60/60 [==============================] - 4s 62ms/step - loss: 0.2265 - accuracy: 0.9177\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 52s 206ms/step - loss: 1.0294 - accuracy: 0.5298 - val_loss: 1.8433 - val_accuracy: 0.6068\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.6272 - accuracy: 0.7230 - val_loss: 0.5322 - val_accuracy: 0.7812\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.3910 - accuracy: 0.8345 - val_loss: 0.2073 - val_accuracy: 0.9141\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.2686 - accuracy: 0.8994 - val_loss: 0.2021 - val_accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.2182 - accuracy: 0.9219 - val_loss: 0.1776 - val_accuracy: 0.9375\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1977 - accuracy: 0.9272 - val_loss: 0.0788 - val_accuracy: 0.9766\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1642 - accuracy: 0.9422 - val_loss: 0.2795 - val_accuracy: 0.8958\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1570 - accuracy: 0.9432 - val_loss: 0.1793 - val_accuracy: 0.9323\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1328 - accuracy: 0.9558 - val_loss: 0.0805 - val_accuracy: 0.9740\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1145 - accuracy: 0.9602 - val_loss: 0.0978 - val_accuracy: 0.9740\n",
      "216/216 [==============================] - 13s 58ms/step - loss: 0.0410 - accuracy: 0.9884\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0795 - accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 51s 205ms/step - loss: 1.0826 - accuracy: 0.5327 - val_loss: 39.5252 - val_accuracy: 0.2526\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.6752 - accuracy: 0.7331 - val_loss: 0.4813 - val_accuracy: 0.8047\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 198ms/step - loss: 0.4540 - accuracy: 0.8088 - val_loss: 0.1943 - val_accuracy: 0.9323\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.3013 - accuracy: 0.8924 - val_loss: 0.2797 - val_accuracy: 0.9062\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.2572 - accuracy: 0.9032 - val_loss: 0.1495 - val_accuracy: 0.9375\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.1648 - accuracy: 0.9418 - val_loss: 0.1627 - val_accuracy: 0.9401\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1787 - accuracy: 0.9323 - val_loss: 0.1189 - val_accuracy: 0.9479\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1616 - accuracy: 0.9467 - val_loss: 0.1570 - val_accuracy: 0.9427\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1188 - accuracy: 0.9588 - val_loss: 0.2713 - val_accuracy: 0.9427\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1297 - accuracy: 0.9477 - val_loss: 0.0800 - val_accuracy: 0.9661\n",
      "216/216 [==============================] - 13s 59ms/step - loss: 0.0420 - accuracy: 0.9881\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0973 - accuracy: 0.9667 0s\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 52s 206ms/step - loss: 1.0615 - accuracy: 0.5216 - val_loss: 20.8996 - val_accuracy: 0.2865\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.5067 - accuracy: 0.8031 - val_loss: 0.3308 - val_accuracy: 0.8958\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 45s 207ms/step - loss: 0.3683 - accuracy: 0.8710 - val_loss: 0.3306 - val_accuracy: 0.8828\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.2782 - accuracy: 0.8945 - val_loss: 0.1874 - val_accuracy: 0.9297\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2492 - accuracy: 0.9102 - val_loss: 0.1621 - val_accuracy: 0.9427\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1740 - accuracy: 0.9399 - val_loss: 0.1063 - val_accuracy: 0.9609\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1867 - accuracy: 0.9340 - val_loss: 0.1402 - val_accuracy: 0.9583\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1293 - accuracy: 0.9531 - val_loss: 0.1628 - val_accuracy: 0.9401\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.1104 - accuracy: 0.9564 - val_loss: 0.5413 - val_accuracy: 0.9219\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.0877 - accuracy: 0.9721 - val_loss: 0.0779 - val_accuracy: 0.9661\n",
      "216/216 [==============================] - 13s 61ms/step - loss: 0.0272 - accuracy: 0.9928\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 0.0840 - accuracy: 0.9760\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 51s 209ms/step - loss: 1.1348 - accuracy: 0.5051 - val_loss: 1.0561 - val_accuracy: 0.5781\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.6587 - accuracy: 0.7042 - val_loss: 0.5668 - val_accuracy: 0.7396\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.4124 - accuracy: 0.8468 - val_loss: 0.2919 - val_accuracy: 0.8750\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2781 - accuracy: 0.9046 - val_loss: 0.2101 - val_accuracy: 0.9271\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2502 - accuracy: 0.9091 - val_loss: 0.1440 - val_accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1766 - accuracy: 0.9394 - val_loss: 0.1774 - val_accuracy: 0.9349\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1525 - accuracy: 0.9448 - val_loss: 0.2124 - val_accuracy: 0.9271\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1397 - accuracy: 0.9528 - val_loss: 0.1477 - val_accuracy: 0.9401\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1144 - accuracy: 0.9602 - val_loss: 0.0814 - val_accuracy: 0.9609\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1210 - accuracy: 0.9589 - val_loss: 0.0847 - val_accuracy: 0.9714\n",
      "216/216 [==============================] - 13s 59ms/step - loss: 0.0423 - accuracy: 0.9893\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0831 - accuracy: 0.9771\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 50s 205ms/step - loss: 1.1501 - accuracy: 0.5257 - val_loss: 1.3510 - val_accuracy: 0.5729\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.6107 - accuracy: 0.7757 - val_loss: 0.3774 - val_accuracy: 0.8490\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 43s 199ms/step - loss: 0.4714 - accuracy: 0.8232 - val_loss: 0.4066 - val_accuracy: 0.8750\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.3452 - accuracy: 0.8753 - val_loss: 0.2048 - val_accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 44s 204ms/step - loss: 0.2862 - accuracy: 0.8965 - val_loss: 0.3482 - val_accuracy: 0.9010\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2365 - accuracy: 0.9144 - val_loss: 0.3803 - val_accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1946 - accuracy: 0.9283 - val_loss: 0.4342 - val_accuracy: 0.9193\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1683 - accuracy: 0.9349 - val_loss: 0.1363 - val_accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.1499 - accuracy: 0.9511 - val_loss: 0.2204 - val_accuracy: 0.9271\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.1419 - accuracy: 0.9553 - val_loss: 0.2397 - val_accuracy: 0.9557\n",
      "216/216 [==============================] - 13s 61ms/step - loss: 0.2744 - accuracy: 0.9563\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.4456 - accuracy: 0.9375\n",
      "3456\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 51s 209ms/step - loss: 1.0967 - accuracy: 0.5258 - val_loss: 4.5957 - val_accuracy: 0.5833\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 44s 202ms/step - loss: 0.6247 - accuracy: 0.7377 - val_loss: 0.6061 - val_accuracy: 0.7109\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.4089 - accuracy: 0.8383 - val_loss: 0.3589 - val_accuracy: 0.8854\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.3346 - accuracy: 0.8691 - val_loss: 0.3952 - val_accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2717 - accuracy: 0.8985 - val_loss: 0.2580 - val_accuracy: 0.8958\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 44s 203ms/step - loss: 0.2455 - accuracy: 0.9077 - val_loss: 0.1233 - val_accuracy: 0.9635\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 43s 201ms/step - loss: 0.2030 - accuracy: 0.9250 - val_loss: 0.2805 - val_accuracy: 0.9271\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1839 - accuracy: 0.9349 - val_loss: 0.1211 - val_accuracy: 0.9531\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1519 - accuracy: 0.9464 - val_loss: 0.0743 - val_accuracy: 0.9766\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 43s 200ms/step - loss: 0.1170 - accuracy: 0.9554 - val_loss: 0.0861 - val_accuracy: 0.9635\n",
      "216/216 [==============================] - 13s 59ms/step - loss: 0.0381 - accuracy: 0.9899\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0739 - accuracy: 0.9802 0s - loss: 0.076\n"
     ]
    }
   ],
   "source": [
    "image_size = 227\n",
    "\n",
    "groups_folder_path = '../Data/resultVecsFigs/AAFT_3/'\n",
    "\n",
    "X_s = []\n",
    "\n",
    "y = []\n",
    "# auto_corr   hadamard     midAng    striping_color   striping_bw     striping_midAng_color     striping_midAng_bw\n",
    "for label in ['Bi', 'Tri', 'Hammer', 'Rvcurl']:\n",
    "\n",
    "    for feature in ['hadamard']:\n",
    "\n",
    "        for top, dir, f in os.walk(groups_folder_path + label + \"/\" + feature + \"/\"):\n",
    "            # print(f)\n",
    "            for filename in f:\n",
    "                # print(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\n",
    "                img = cv2.imread(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\n",
    "                img = cv2.resize(img, None, fx=image_size / img.shape[0], fy=image_size / img.shape[1])\n",
    "\n",
    "                X_s.append(img / 256)\n",
    "\n",
    "                if label == 'Bi':\n",
    "                    y.append(0)\n",
    "                elif label =='Tri':\n",
    "                    y.append(1)\n",
    "                elif label == 'Hammer':\n",
    "                    y.append(2)\n",
    "                else:\n",
    "                    y.append(3)\n",
    "\n",
    "print(X_s[0].shape)\n",
    "# print(X_s)\n",
    "print(len(y))\n",
    "X = np.array(X_s)\n",
    "y = np.array(y)\n",
    "\n",
    "ins = []\n",
    "for i in range(10):\n",
    "    ins.append(inceptionV4_model(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39526689052581787\n",
      "0.028467146679759026\n",
      "0.047630660235881805\n",
      "0.173666313290596\n",
      "0.040956202894449234\n",
      "0.04204784333705902\n",
      "0.02722274325788021\n",
      "0.04231007769703865\n",
      "0.27440640330314636\n",
      "0.038113921880722046\n"
     ]
    }
   ],
   "source": [
    "for data in ins:\n",
    "    print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hispanic-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666865348816\n",
      "0.9750000238418579\n",
      "0.9572916626930237\n",
      "0.9177083373069763\n",
      "0.96875\n",
      "0.9666666388511658\n",
      "0.9760416746139526\n",
      "0.9770833253860474\n",
      "0.9375\n",
      "0.9802083373069763\n"
     ]
    }
   ],
   "source": [
    "for data in ins:\n",
    "    print(data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appreciated-latitude",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngroups_folder_path = \\'../Data/resultVecsFigs/AAFT_4/\\'\\nX_s = []\\ny = []\\n# auto_corr   hadamard     midAng    striping_color   striping_bw     striping_midAng_color     striping_midAng_bw\\nfor label in [\\'Bi\\', \\'Tri\\']:\\n\\n    for feature in [\\'hadamard\\']:\\n\\n        for top, dir, f in os.walk(groups_folder_path + label + \"/\" + feature + \"/\"):\\n            # print(f)\\n            for filename in f:\\n                # print(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\\n                img = cv2.imread(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\\n                img = cv2.resize(img, None, fx=image_size / img.shape[0], fy=image_size / img.shape[1])\\n\\n                X_s.append(img / 256)\\n\\n                if label == \\'Bi\\':\\n                    y.append(0)\\n                else:\\n                    y.append(1)\\n\\nprint(X_s[0].shape)\\n# print(X_s)\\nprint(len(y))\\nX = np.array(X_s)\\ny = np.array(y)\\n\\nins = []\\nfor i in range(10):\\n    ins.append(inceptionV4_model(X, y))\\n\\nfor data in ins:\\n    for d in data:\\n        for j in d:\\n            print(j)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "groups_folder_path = '../Data/resultVecsFigs/AAFT_4/'\n",
    "X_s = []\n",
    "y = []\n",
    "# auto_corr   hadamard     midAng    striping_color   striping_bw     striping_midAng_color     striping_midAng_bw\n",
    "for label in ['Bi', 'Tri']:\n",
    "\n",
    "    for feature in ['hadamard']:\n",
    "\n",
    "        for top, dir, f in os.walk(groups_folder_path + label + \"/\" + feature + \"/\"):\n",
    "            # print(f)\n",
    "            for filename in f:\n",
    "                # print(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\n",
    "                img = cv2.imread(groups_folder_path + label + \"/\" + feature + \"/\" + filename)\n",
    "                img = cv2.resize(img, None, fx=image_size / img.shape[0], fy=image_size / img.shape[1])\n",
    "\n",
    "                X_s.append(img / 256)\n",
    "\n",
    "                if label == 'Bi':\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "\n",
    "print(X_s[0].shape)\n",
    "# print(X_s)\n",
    "print(len(y))\n",
    "X = np.array(X_s)\n",
    "y = np.array(y)\n",
    "\n",
    "ins = []\n",
    "for i in range(10):\n",
    "    ins.append(inceptionV4_model(X, y))\n",
    "\n",
    "for data in ins:\n",
    "    for d in data:\n",
    "        for j in d:\n",
    "            print(j)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
